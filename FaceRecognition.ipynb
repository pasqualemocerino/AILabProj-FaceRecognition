{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT0bcLf0d06/5PDr99YlPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasqualemocerino/AILabProj-FaceRecognition/blob/main/FaceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Face Recognition\n",
        "Face Recogniton is a form of biometrics, the science which deals with the automated recognition of individuals based on biological and behavioral characteristics. It is included in physiological or static biometrics, based on data derived from the measurement of a part of a person's anatomy.\n",
        "The main purpose is the authentication of the subject, which follows identification and is based on proving the previously declared identity.\n",
        "We will later discuss about the advantages and disadvantages of this mechanism, analysing the requirements for an ideal biometric identifier:\n",
        "1. Universality\n",
        "2. Uniqueness\n",
        "3. Performance\n",
        "4. Collectability\n",
        "5. Acceptability\n",
        "\n",
        "Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\n"
      ],
      "metadata": {
        "id": "ey0_FL5hqG0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###About the dataset\n",
        "Labeled Faces in the Wild (LFW) is an image dataset containing face photographs, collected especially for studying the problem of unconstrained face recognition. It includes over 13,000 images of faces collected from across the web. Here are key aspects of these images.\n",
        "\n",
        "Each face in this data set was labeled with the person’s name in the image.\n",
        "1680 of the photographed persons distinctly appear in two or more photos in the data set.\n",
        "The faces in these images were detected by the Viola-Jones face detector (Paul Viola and Michael Jones, 2001).\n",
        "\n",
        "LFW includes four different sets of images, including the original and three types of aligned images that can be used to test algorithms under different conditions. For alignment, the dataset uses funneled images (ICCV 2007), LFW-a, and deep funneled images (NIPS 2012). Deep funneled and LFW-a images produce superior results for most face verification algorithms over the funneled images and the original images.\n",
        "\n",
        "Face Recognition is a task typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites."
      ],
      "metadata": {
        "id": "G4qDuODxas12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Purpose\n",
        "Our first purpose is to train an image classifier to work on a dataset of well-known faces of famous people. Then we will make the neural network recognize further faces extending the dataset with new measurements linked to proper identity labels. The final objective is to make an app which recognize human faces after collecting data."
      ],
      "metadata": {
        "id": "dIn508Tuslbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Schedule\n",
        "1. Exploratory Data Analysis \n",
        "2. Define the relevant metrics to be used\n",
        "3. Train a first baseline algorithm as a reference\n",
        "4. Prepare data where needed\n",
        "5. Design experiments and define hyperparameters \n",
        "6. Repeat until performance on the test set is acceptable"
      ],
      "metadata": {
        "id": "sEvdbZKHugMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exploratory Data Analysis \n",
        "* Regression or Classification?\n",
        "We are obiouvsly dealing with a problem of classification of faces regarding their labels.\n",
        "\n",
        "* What is the target variable?\n",
        "The target variable of a dataset is the feature of a dataset about which you want to gain a deeper understanding. A supervised machine learning algorithm uses historical data to learn patterns and uncover relationships between other features of your dataset and the target.\n",
        "Targets are labeled in the dataset and consists in the identities associated with famous faces.\n",
        "\n",
        "* Is the data unbalanced? Dataset is unbalanced, there is a prevalence of images of single subjects, which could imply bias.\n",
        "* What are the features? Correlation, ranges, variances, NaN, errors...\n",
        "* Plot to make findings clearer\n"
      ],
      "metadata": {
        "id": "TvvJx6stvGhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 5749 unique celebrities in the entire dataset, of whom 1680 are represented by multiple images. The entire number of images available is 13233. The most represented celebrity is George_W_Bush, with 530 unique images in the dataset."
      ],
      "metadata": {
        "id": "ZZRmDtNi2uNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the relevant metrics to be used"
      ],
      "metadata": {
        "id": "Sdcg9o0yvhsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train a first baseline algorithm as a reference\n",
        "For classification, train basic models as the one-class-classifier and the basic logistic regression with all the features."
      ],
      "metadata": {
        "id": "LcjtmLzmvyrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare data where needed\n",
        "* Cleaning\n",
        "* Normalization \n",
        "* Shuffling and train, test and validation set construction (check the statistical properties of the splits)"
      ],
      "metadata": {
        "id": "sG0M4c5gwGM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Design experiments and define hyperparameters"
      ],
      "metadata": {
        "id": "rSDQjhFawW7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Repeat until performance on the test set is acceptable \n",
        "* Train model and cross validate hyperparameters until acceptable performance on training set is achieved\n",
        "* Test best hyperparameter model and check if there is overfitting or underfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "c1OWBFe3wcZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an image classifier\n",
        "We will do the following steps in order:\n",
        "1. Load and normalize the training test datasets using ``torchvision``\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function and an optimizer\n",
        "4. Train the network on the training data \n",
        "5. Test the network on the test data "
      ],
      "metadata": {
        "id": "M3rWL-jbKy_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Ysk2HAPTK_P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "ETFJfd2uLwBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Load and normalize training test datasets\n",
        "(The output of torchvision datasets are PILImage of range [0,1]. We transform them to Tensors of normalized range [-1,1].)"
      ],
      "metadata": {
        "id": "czmsILhAL8Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training hyperparameters\n",
        "INIT_LR = 0.001\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "\n",
        "#Setting the device to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Loading the dataset\n",
        "print(\"[INFO] Loading the LFW dataset...\")\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize((50,50)),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.LFWPeople(root='./data', split='train', \n",
        "                          transform = transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "print(\"[INFO] Trainset loaded.\")\n",
        "testset = torchvision.datasets.LFWPeople(root='./data', split='test',\n",
        "                          transform = transform, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "print(\"[INFO] Testset loaded.\")"
      ],
      "metadata": {
        "id": "yAFfYs_eMQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=1)\n",
        "classes = list(lfw_people.target_names)\n",
        "num_classes = len(classes)\n",
        "print(num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLlxWQnGvFPF",
        "outputId": "3d63a211-4669-434b-845f-5ac7eb8084e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "  img = img/2 + 0.5 #unnormalize\n",
        "  plt.imshow(np.transpose(img.numpy(), (1,2,0)))\n",
        "\n",
        "trainiter = iter(trainloader)\n",
        "images, labels = trainiter.next()\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "for j in range(BATCH_SIZE):\n",
        "  if (j != 0 and j % 8 == 0): print()\n",
        "  print(classes[labels[j]].ljust(25), end=\" \")\n",
        "  "
      ],
      "metadata": {
        "id": "tiICyuD_vRW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Define a Convolutional Neural Network \n",
        "The CNN is created from scratch to take 3-channel images."
      ],
      "metadata": {
        "id": "qvw_zhycNTW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module): \n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      #conv1, conv2, fc1, fc2, fc3\n",
        "\n",
        "    def forward(self, x):\n",
        "      #...\n",
        "      return x"
      ],
      "metadata": {
        "id": "09Zh1sPQNgqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Define a loss function and an optimizer\n",
        "We use a Classification Cross-Entropy loss and SGD with momentum."
      ],
      "metadata": {
        "id": "Gn_M6vmHOvbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim \n",
        "\n",
        "\n",
        "model = ConvNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=INIT_LR, momentum=0.9)\n",
        "#criterion = \n",
        "#optimizer = "
      ],
      "metadata": {
        "id": "WYFJRsPIPL97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Train the network on the training data\n",
        "We have to loop over our data iterator and feed the inputs to the network and optimize."
      ],
      "metadata": {
        "id": "zPOvqV_MPYmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      inputs, labels = data\n",
        "\n",
        "      #Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #Forward, backward and optimization\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 50 == 49:\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / 50))\n",
        "        running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "izcndweRPyH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Test the network on the test data \n",
        "Check if the network has learnt anything at all."
      ],
      "metadata": {
        "id": "ZZbgsOQyQ_lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A better approach is to work with a network which follows the idea behind FaceNet, working on embeddings of images."
      ],
      "metadata": {
        "id": "G63sIiork6uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FaceNet\n",
        "FaceNet is a deep neural network used for extracting features from an image of a person’s face. It was published in 2015 by Google researchers.\n",
        "\n",
        "FaceNet takes an image of the person’s face as input and outputs a vector of 128 numbers which represent the most important features of a face. In machine learning, this vector is called embedding. Why embedding? Because all the important information from an image is embedded into this vector. Basically, FaceNet takes a person’s face and compresses it into a vector of 128 numbers. Ideally, embeddings of similar faces are also similar.\n",
        "\n",
        "FaceNet learns in the following way:\n",
        "\n",
        "1. Randomly selects an anchor image.\n",
        "2. Randomly selects an image of the same person as the anchor image (positive example).\n",
        "3. Randomly selects an image of a person different than the anchor image (negative example).\n",
        "4. Adjusts the FaceNet network parameters so that the positive example is closer to the anchor than the negative example.\n",
        "\n",
        "We repeat these steps until there are no more changes to be done, so all the faces of the same person are close to each other and far from others.\n",
        "\n",
        "This method of learning with anchor, positive and negative examples is called triplet loss.\n",
        "\n",
        "The classification step could be done by calculating the embedding distances between a new face and known faces, but that approach is too computationally and memory expensive (this approach is called k-NN). Instead, we decided to use the Softmax classifier which memorises boundaries between people which is much more efficient."
      ],
      "metadata": {
        "id": "X9CxTHxLlNlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Schedule\n",
        "1. Pre-processing — a method used to take a set of images and convert them all to a uniform format — in our case, a square image containing just a person’s face. A uniform dataset is useful for decreasing variance when training as we have limited computational resources when using the Edge TPU.\n",
        "2. Embedding — a process, fundamental to the way FaceNet works, which learns representations of faces in a multidimensional space where distance corresponds to a measure of face similarity.\n",
        "3. Classification — the final step which uses information given by the embedding process to separate distinct faces."
      ],
      "metadata": {
        "id": "R4HsHr90lO4l"
      }
    }
  ]
}